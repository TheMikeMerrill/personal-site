@poster{frank2017sensing,
  title={Sensing depression: Using smartphone sensors to predict changes in depression severity},
  author={Frank, Ellen and Merrill, Michael and Aung, Hane and Soares, Claudio and Kennedy, Sidney and Matthews, Mark and Kupfer, David and Lalovic, Aleksandra and Choudhury, Tanzeem},
  booktitle={Neuropsychopharmacology},
  volume={43},
  pages={S346--S346},
  year={2017},
  organization={NATURE PUBLISHING GROUP MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
  venue= {The 56th Annual Meeting of the American College of Neuropsychopharmacology}
}


@poster{frank2016sensing,
  title={Continuous Behavioral Data as a Potential Depression Biomarker},
  author={Frank, Ellen and Merrill, Michael and Aung, Hane and Soares, Claudio and Kennedy, Sidney and Matthews, Mark and Kupfer, David and Lalovic, Aleksandra and Choudhury, Tanzeem},
  booktitle={Neuropsychopharmacology},
  year = {2016},
  organization={NATURE PUBLISHING GROUP MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
  venue= {The 56th Annual Meeting of the American College of Neuropsychopharmacology}
}

@inproceedings{tseng2016ubicomp,
    author = {Tseng, Vincent W. S. and Merrill, Michael and Wittleder, Franziska and Abdullah, Saeed and Aung, Min Hane and Choudhury, Tanzeem},
    title = {Assessing Mental Health Issues on College Campuses: Preliminary Findings from a Pilot Study},
    year = {2016},
    isbn = {9781450344623},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2968219.2968308},
    doi = {10.1145/2968219.2968308},
    booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
    pages = {1200–1208},
    numpages = {9},
    keywords = {predictive modeling, mobile sensing, mental health, behavioral intervention, mHealth},
    location = {Heidelberg, Germany},
    series = {UbiComp ’16},
	venue = {Ubicomp}
}

@article{ben-zeev_crosscheck_2017,
	title = {{CrossCheck}: {Integrating} self-report, behavioral sensing, and smartphone use to identify digital indicators of psychotic relapse},
	volume = {40},
	issn = {1095-158X},
	shorttitle = {{CrossCheck}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5593755/},
	doi = {10.1037/prj0000243}, 
	abstract = {Objective
                This purpose of this study was to describe and demonstrate CrossCheck, a multimodal data collection system designed to aid in continuous remote monitoring and identification of subjective and objective indicators of psychotic relapse
                Methods
                Individuals with schizophrenia-spectrum disorders received a smartphone with the monitoring system installed along with unlimited data plan for 12 months. Participants were instructed to carry the device with them and to complete brief self-reports multiple times a week. Multi-modal behavioral sensing (i.e., physical activity, geospatial activity, speech frequency and duration) and device use data (i.e., call and text activity, app use) were captured automatically. Five individuals who experienced psychiatric hospitalization were selected and described for instructive purposes.

                Results
                Participants had unique digital indicators of their psychotic relapse. For some, self-reports provided clear and potentially actionable description of symptom exacerbation prior to hospitalization. Others had behavioral sensing data trends (e.g., shifts in geolocation patterns, declines in physical activity) or device use patterns (e.g., increased nighttime app use, discontinuation of all smartphone use) that reflected the changes they experienced more effectively.

                Conclusion
                Advancements in mobile technology are enabling collection of an abundance of information that until recently was largely inaccessible to clinical research and practice. However, remote monitoring and relapse detection is in its nascency. Development and evaluation of innovative data management, modeling, and signal-detection techniques that can identify changes within an individual over time (i.e. unique relapse signatures) will be essential if we are to capitalize on these data to improve treatment and prevention.},
	number = {3},
	urldate = {2020-06-25},
	journal = {Psychiatric Rehabilitation Journal},
	venue = {Psychiatric Rehabilitation Journal},
	author = {Ben-Zeev, Dror and Brian, Rachel and Wang, Rui and Wang, Weichen and Campbell, Andrew T. and Aung, Min S. H. and Merrill, Michael and Tseng, Vincent W. S. and Choudhury, Tanzeem and Hauser, Marta and Kane, John M. and Scherer, Emily A.},
	month = sep,
	year = {2017},
	pmid = {28368138},
	pmcid = {PMC5593755},
	pages = {266--275},
	file = {PubMed Central Full Text PDF:/Users/michaelmerrill/Zotero/storage/BM5A6Q6N/Ben-Zeev et al. - 2017 - CrossCheck Integrating self-report, behavioral se.pdf:application/pdf}
}


@inproceedings{wang_crosscheck_2016,
	address = {Heidelberg Germany},
	title = {{CrossCheck}: toward passive sensing and detection of mental health changes in people with schizophrenia},
	isbn = {978-1-4503-4461-6},
	shorttitle = {{CrossCheck}},
	url = {https://dl.acm.org/doi/10.1145/2971648.2971740},
	doi = {10.1145/2971648.2971740},
	abstract = {Early detection of mental health changes in individuals with serious mental illness is critical for effective intervention. CrossCheck is the ﬁrst step towards the passive monitoring of mental health indicators in patients with schizophrenia and paves the way towards relapse prediction and early intervention. In this paper, we present initial results from an ongoing randomized control trial, where passive smartphone sensor data is collected from 21 outpatients with schizophrenia recently discharged from hospital over a period ranging from 2-8.5 months. Our results indicate that there are statistically signiﬁcant associations between automatically tracked behavioral features related to sleep, mobility, conversations, smartphone usage and self-reported indicators of mental health in schizophrenia. Using these features we build inference models capable of accurately predicting aggregated scores of mental health indicators in schizophrenia with a mean error of 7.6\% of the score range. Finally, we discuss results on the level of personalization that is needed to account for the known variations within people. We show that by leveraging knowledge from a population with schizophrenia, it is possible to train accurate personalized models that require fewer individual-speciﬁc data to quickly adapt to new users.},
	language = {en},
	urldate = {2020-06-25},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {ACM},
	author = {Wang, Rui and Aung, Min S. H. and Abdullah, Saeed and Brian, Rachel and Campbell, Andrew T. and Choudhury, Tanzeem and Hauser, Marta and Kane, John and Merrill, Michael and Scherer, Emily A. and Tseng, Vincent W. S. and Ben-Zeev, Dror},
	month = sep,
	year = {2016},
	pages = {886--897},
	file = {Wang et al. - 2016 - CrossCheck toward passive sensing and detection o.pdf:/Users/michaelmerrill/Zotero/storage/88HEZH9G/Wang et al. - 2016 - CrossCheck toward passive sensing and detection o.pdf:application/pdf},
	venue = {Ubicomp}
}




@article{zhang2022coral,
	Author = {Zhang, Ge and Merrill, Mike A. and Liu, Yang and Heer, Jeffrey and Althoff, Tim},
	venue = {EPJ Data Science},
	Number = {1},
	Pages = {14},
	Title = {{CORAL: COde RepresentAtion Learning with Weakly-Supervised Transformers for Analyzing Data Analysis}},
	Volume = {11},
    notes = {*Co-First Author},
	Year = {2022}}


@misc{merrill2022selfsupervised,
  doi = {10.48550/ARXIV.2205.13607},
  url = {https://arxiv.org/abs/2205.13607},
  author = {Merrill, Mike A. and Althoff, Tim},
  keywords = {Machine Learning (cs.LG), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {{Self-supervised Pretraining and Transfer Learning Enable Flu and COVID-19 Predictions in Small Mobile Sensing Datasets}},
  publisher = {arXiv},
  journal = {CHIL},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{merrill2021multiverse,
	title = {{MULTIVERSE: Mining Collective Data Science Knowledge from
	Code on the Web to Suggest Alternative Analysis Approaches}},
	shorttitle = {{CrossCheck}},
	journal = {KDD},
	urldate = {2020-06-25},
	venue = {KDD},
	author = {{Merrill}, {Mike A.} and Zhang, Ge  and  Althoff, Tim},
	year = {2021}
}


@misc{xu2022GLOBEM,
  doi = {10.48550/ARXIV.2211.02733},
  
  url = {https://arxiv.org/abs/2211.02733},
  
  author = {Xu, Xuhai and Zhang, Han and Sefidgar, Yasaman and Ren, Yiyi and Liu, Xin and Seo, Woosuk and Brown, Jennifer and Kuehn, Kevin and Merrill, {Mike A.} and Nurius, Paula and Patel, Shwetak and Althoff, Tim and Morris, Margaret E. and Riskin, Eve and Mankoff, Jennifer and Dey, Anind K.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.1; E.m, 68T09},
  
  title = {GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization},
  
  publisher = {arXiv},
  journal={NeurIPS},
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{merrillHomekit2020BenchmarkTime2023,
  title = {Homekit2020: {{A Benchmark}} for {{Time Series Classification}} on a {{Large Mobile Sensing Dataset}} with {{Laboratory Tested Ground Truth}} of {{Influenza Infections}}},
  author = {Merrill, Mike A and Safranchik, Esteban and Kolbeinsson, Arinbjorn and Gade, Piyusha and Ramirez, Ernesto and Schmidt, Ludwig and Foshchini, Luca and Althoff, Tim},
  date = {2023-03-26},
  journaltitle = {Conference on Health, Inference, and Learning},
  shortjournal = {CHIL},
  abstract = {Despite increased interest in wearables as tools for detecting various health conditions, there are not as of yet any large public benchmarks for such mobile sensing data. The few datasets that are available do not contain data from more than dozens of individuals, do not contain high-resolution raw data or do not include dataloaders for easy integration into machine learning pipelines. Here, we present Homekit2020: the first large-scale public benchmark for time series classification of wearable sensor data. Our dataset contains over 14 million hours of minute-level multimodal FitBit data, symptom reports, and ground-truth laboratory PCR influenza test results, along with an evaluation framework that mimics realistic model deployments and efficiently characterizes statistical uncertainty in model selection in the presence of extreme class imbalance. Furthermore, we implement and evaluate nine neural and non-neural time series classification models on our benchmark across 450 total training runs in order to establish state of the art performance.},
  langid = {english},
}

@online{merrillLanguageModelsStill2024,
  title = {Language {{Models Still Struggle}} to {{Zero-shot Reason}} about {{Time Series}}},
  author = {Merrill, Mike A. and Tan, Mingtian and Gupta, Vinayak and Hartvigsen, Tom and Althoff, Tim},
  date = {2024-04-17},
  eprint = {2404.11757},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.11757},
  urldate = {2024-05-14},
  abstract = {Time series are critical for decision-making in fields like finance and healthcare. Their importance has driven a recent influx of works passing time series into language models, leading to non-trivial forecasting on some datasets. But it remains unknown whether non-trivial forecasting implies that language models can reason about time series. To address this gap, we generate a first-of-its-kind evaluation framework for time series reasoning, including formal tasks and a corresponding dataset of multi-scale time series paired with text captions across ten domains. Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning—given an input time series, can the language model identify the scenario that most likely created it? (2) Question Answering—can a language model answer factual questions about time series? (3) Context-Aided Forecasting—does highly relevant textual context improve a language model’s time series forecasts? We find that otherwise highly-capable language models demonstrate surprisingly limited time series reasoning: they score marginally above random on etiological and question answering tasks (up to 30 percentage points worse than humans) and show modest success in using context to improve forecasting. These weakness showcase that time series reasoning is an impactful, yet deeply underdeveloped direction for language model research. We also make our datasets and code public at to support further research in this direction at https://github.com/behavioral-data/TSandLanguage.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/michaelmerrill/Zotero/storage/DC4WMC7Y/Merrill et al. - 2024 - Language Models Still Struggle to Zero-shot Reason.pdf}
}

@online{tanAreLanguageModels2024,
  title = {Are {{Language Models Actually Useful}} for {{Time Series Forecasting}}?},
  author = {Tan, Mingtian and Merrill, Mike A. and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
  date = {2024-06-21},
  eprint = {2406.16964},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2406.16964},
  urldate = {2024-06-28},
  abstract = {Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/michaelmerrill/Zotero/storage/VN62QUH7/Tan et al. - 2024 - Are Language Models Actually Useful for Time Serie.pdf}
}

@online{merrillTransformingWearableData2024a,
  title = {Transforming {{Wearable Data}} into {{Health Insights}} Using {{Large Language Model Agents}}},
  author = {Merrill, Mike A. and Paruchuri, Akshay and Rezaei, Naghmeh and Kovacs, Geza and Perez, Javier and Liu, Yun and Schenck, Erik and Hammerquist, Nova and Sunshine, Jake and Tailor, Shyam and Ayush, Kumar and Su, Hao-Wei and He, Qian and McLean, Cory Y. and Malhotra, Mark and Patel, Shwetak and Zhan, Jiening and Althoff, Tim and McDuff, Daniel and Liu, Xin},
  date = {2024-06-11},
  eprint = {2406.06464},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2406.06464},
  urldate = {2024-06-28},
  abstract = {Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data. The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale. Yet, the application of LLM agents in analyzing personal health is still largely untapped. In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables. We curate two benchmark question answering datasets of over 4000 health insights questions. Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84\% of factual numerical questions and more than 83\% of crowd-sourced open-ended questions. This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/michaelmerrill/Zotero/storage/6FHQRX29/Merrill et al. - 2024 - Transforming Wearable Data into Health Insights us.pdf}
}



@online{guBLADEBenchmarkingLanguage2024a,
  title = {{{BLADE}}: {{Benchmarking Language Model Agents}} for {{Data-Driven Science}}},
  shorttitle = {{{BLADE}}},
  author = {Gu, Ken and Shang, Ruoxi and Jiang, Ruien and Kuang, Keying and Lin, Richard-John and Lyu, Donghe and Mao, Yue and Pan, Youran and Wu, Teng and Yu, Jiaqian and Zhang, Yikun and Zhang, Tianmai M. and Zhu, Lanyi and Merrill, Mike A. and Heer, Jeffrey and Althoff, Tim},
  date = {2024-08-20},
  eprint = {2408.09667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2408.09667},
  urldate = {2024-09-30},
  abstract = {Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents’ multifaceted approaches to openended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents’ analysis approaches.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/michaelmerrill/Zotero/storage/NL25CLMM/Gu et al. - 2024 - BLADE Benchmarking Language Model Agents for Data.pdf}
}
