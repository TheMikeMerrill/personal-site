---
abstract: Large language models (LLMs) are being applied to time series tasks, particularly
  time series forecasting. However, are language models actually useful for time series?
  After a series of ablation studies on three recent and popular LLM-based time series
  forecasting methods, we find that removing the LLM component or replacing it with
  a basic attention layer does not degrade the forecasting results -- in most cases
  the results even improved. We also find that despite their significant computational
  cost, pretrained LLMs do no better than models trained from scratch, do not represent
  the sequential dependencies in time series, and do not assist in few-shot settings.
  Additionally, we explore time series encoders and reveal that patching and attention
  structures perform similarly to state-of-the-art LLM-based forecasters.
author: Mingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen
date: '2024-06-21'
year: '2024'
eprint: '2406.16964'
eprintclass: cs,
venue: NeurIPS [Spotlight ðŸ”Ž]
eprinttype: arxiv
file: /Users/michaelmerrill/Zotero/storage/VN62QUH7/Tan et al. - 2024 - Are Language
  Models Actually Useful for Time Serie.pdf
key: tanAreLanguageModels2024
keywords: Computer Science - Artificial Intelligence,Computer Science - Machine Learning
langid: english
pdf_path: resources/pubpdfs/tanAreLanguageModels2024.pdf
pubstate: preprint
thumb_path: resources/thumbnails/tanAreLanguageModels2024.png
title: '  Are Language Models Actually Useful for Time Series Forecasting?'
url: http://arxiv.org/abs/2406.16964
urldate: '2024-06-28'
---